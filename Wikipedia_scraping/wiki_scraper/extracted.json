[
{"https://en.wikipedia.org/wiki/Thesaurus_(information_retrieval)": ["Pages for logged out editors learn moreA language model is a probability distribution over sequences of words ", " 1 Given any sequence of words of length m a language model assigns a probability P w 1 w m displaystyle P w 1 ldots w m to the whole sequence ", "Language models generate probabilities by training on text corpora in one or many languages ", "Given that languages can be used to express an infinite variety of valid sentences the property of digital infinity language modeling faces the problem of assigning non zero probabilities to linguistically valid sequences that may never be encountered in the training data ", "Several modelling approaches have been designed to surmount this problem such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers ", "Language models are useful for a variety of problems in computational linguistics from initial applications in speech recognition 2 to ensure nonsensical i e ", "low probability word sequences are not predicted to wider use in machine translation 3 e g ", "scoring candidate translations natural language generation generating more human like text part of speech tagging parsing 3 Optical Character Recognition handwriting recognition 4 grammar induction 5 information retrieval 6 7 and other applications ", "Language models are used in information retrieval in the query likelihood model ", "There a separate language model is associated with each document in a collection ", "Documents are ranked based on the probability of the query Q in the document s language model M d displaystyle M d P Q M d displaystyle P Q mid M d ", "Commonly the unigram language model is used for this purpose ", "A unigram model can be treated as the combination of several one state finite automata ", " 8 It assumes that the probabilities of tokens in a sequence are independent e g ", " In this model the probability of each word only depends on that word s own probability in the document so we only have one state finite automata as units ", "The automaton itself has a probability distribution over the entire vocabulary of the model summing to 1 ", "The following is an illustration of a unigram model of a document ", "The probability generated for a specific query is calculated asDifferent documents have unigram models with different hit probabilities of words in it ", "The probability distributions from different documents are used to generate hit probabilities for each query ", "Documents can be ranked for a query according to the probabilities ", "Example of unigram models of two documents In information retrieval contexts unigram language models are often smoothed to avoid instances where P term 0 ", "A common approach is to generate a maximum likelihood model for the entire collection and linearly interpolate the collection model with a maximum likelihood model for each document to smooth the model ", " 9 In an n gram model the probability P w 1 w m displaystyle P w 1 ldots w m of observing the sentence w 1 w m displaystyle w 1 ldots w m is approximated asIt is assumed that the probability of observing the ith word wi in the context history of the preceding i 1 words can be approximated by the probability of observing it in the shortened context history of the preceding n 1 words nth order Markov property ", "To clarify for n 3 and i 2 we have P w i w i n 1 w i 1 P w 2 w 1 displaystyle P w i mid w i n 1 ldots w i 1 P w 2 mid w 1 ", "The conditional probability can be calculated from n gram model frequency counts The terms bigram and trigram language models denote n gram models with n 2 and n 3 respectively ", " 10 Typically the n gram model probabilities are not derived directly from frequency counts because models derived this way have severe problems when confronted with any n grams that have not been explicitly seen before ", "Instead some form of smoothing is necessary assigning some of the total probability mass to unseen words or n grams ", "Various methods are used from simple add one smoothing assign a count of 1 to unseen n grams as an uninformative prior to more sophisticated models such as Good Turing discounting or back off models ", "Bidirectional representations condition on both pre and post context e g words in all layers ", " 11 In a bigram n 2 language model the probability of the sentence I saw the red house is approximated aswhereas in a trigram n 3 language model the approximation isNote that the context of the first n 1 n grams is filled with start of sentence markers typically denoted s ", "Additionally without an end of sentence marker the probability of an ungrammatical sequence I saw the would always be higher than that of the longer sentence I saw the red house ", "Maximum entropy language models encode the relationship between a word and the n gram history using feature functions ", "The equation iswhere Z w 1 w m 1 displaystyle Z w 1 ldots w m 1 is the partition function a displaystyle a is the parameter vector and f w 1 w m displaystyle f w 1 ldots w m is the feature function ", "In the simplest case the feature function is just an indicator of the presence of a certain n gram ", "It is helpful to use a prior on a displaystyle a or some form of regularization ", "The log bilinear model is another example of an exponential language model ", "Neural language models or continuous space language models use continuous representations or embeddings of words to make their predictions ", " 12 These models make use of neural networks ", "Continuous space embeddings help to alleviate the curse of dimensionality in language modeling as language models are trained on larger and larger texts the number of unique words the vocabulary increases ", " a The number of possible sequences of words increases exponentially with the size of the vocabulary causing a data sparsity problem because of the exponentially many sequences ", "Thus statistics are needed to properly estimate probabilities ", "Neural networks avoid this problem by representing words in a distributed way as non linear combinations of weights in a neural net ", " 13 An alternate description is that a neural net approximates the language function ", "The neural net architecture might be feed forward or recurrent and while the former is simpler the latter is more common ", " example needed citation needed Typically neural net language models are constructed and trained as probabilistic classifiers that learn to predict a probability distributionI e the network is trained to predict a probability distribution over the vocabulary given some linguistic context ", "This is done using standard neural net training algorithms such as stochastic gradient descent with backpropagation ", " 13 The context might be a fixed size window of previous words so that the network predictsfrom a feature vector representing the previous k words ", " 13 Another option is to use future words as well as past words as features so that the estimated probability isThis is called a bag of words model ", "When the feature vectors for the words in the context are combined by a continuous operation this model is referred to as the continuous bag of words architecture CBOW ", " 14 A third option that trains slower than the CBOW but performs slightly better is to invert the previous problem and make a neural network learn the context given a word ", " 14 More formally given a sequence of training words w 1 w 2 w 3 w T displaystyle w 1 w 2 w 3 dots w T one maximizes the average log probabilitywhere k the size of the training context can be a function of the center word w t displaystyle w t ", "This is called a skip gram language model ", " 15 Bag of words and skip gram models are the basis of the word2vec program ", " 16 Instead of using neural net language models to produce actual probabilities it is common to instead use the distributed representation encoded in the networks hidden layers as representations of words each word is then mapped onto an n dimensional real vector called the word embedding where n is the size of the layer just before the output layer ", "The representations in skip gram models have the distinct characteristic that they model semantic relations between words as linear combinations capturing a form of compositionality ", "For example in some such models if v is the function that maps a word w to its n d vector representation thenwhere is made precise by stipulating that its right hand side must be the nearest neighbor of the value of the left hand side ", " 14 15 A positional language model 17 assesses the probability of given words occurring close to one another in a text not necessarily immediately adjacent ", "Similarly bag of concepts models 18 leverage the semantics associated with multi word expressions such as buy christmas present even when they are used in information rich sentences like today I bought a lot of very nice Christmas presents ", "Despite the limited successes in using neural networks 19 authors acknowledge the need for other techniques when modelling sign languages ", "Notable language models include Hugging Face hosts a set of publicly available language models for developers to build applications using machine learning ", "Evaluation of the quality of language models is mostly done by comparison to human created sample benchmarks created from typical language oriented tasks ", "Other less established quality tests examine the intrinsic character of a language model or compare two such models ", "Since language models are typically intended to be dynamic and to learn from data it sees some proposed models investigate the rate of learning e g ", "through inspection of learning curves ", " 32 Various data sets have been developed to use to evaluate language processing systems ", " 11 These include Although contemporary language models such as GPT 3 can be shown to match human performance on some tasks it is not clear they are plausible cognitive models ", "For instance recurrent neural networks have been shown to learn patterns humans do not learn and fail to learn patterns that humans do learn ", " 40 "]},
{"https://en.wikipedia.org/wiki/Lexical_analysis": ["Pages for logged out editors learn moreIn the context of information retrieval a thesaurus plural thesauri is a form of controlled vocabulary that seeks to dictate semantic manifestations of metadata in the indexing of content objects ", "A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects ", "ANSI NISO Z39 19 2005 defines a content object as any item that is to be described for inclusion in an information retrieval system website or other source of information ", " 1 The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object ", " 2 A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject ", "ISO 25964 the international standard for information retrieval thesauri defines a thesaurus as a controlled and structured vocabulary in which concepts are represented by terms organized so that relationships between concepts are made explicit and preferred terms are accompanied by lead in entries for synonyms or quasi synonyms A thesaurus is composed by at least three elements 1 a list of words or terms 2 the relationship amongst the words or terms indicated by their hierarchical relative position e g ", "parent broader term child narrower term synonym etc ", " 3 a set of rules on how to use the thesaurus ", "Wherever there have been large collections of information whether on paper or in computers scholars have faced a challenge in pinpointing the items they seek ", "The use of classification schemes to arrange the documents in order was only a partial solution ", "Another approach was to index the contents of the documents using words or terms rather than classification codes ", "In the 1940s and 1950s some pioneers such as Calvin Mooers Charles L Bernier Evan J Crane and Hans Peter Luhn collected up their index terms in various kinds of list that they called a thesaurus by analogy with the well known thesaurus developed by Peter Roget ", " 3 The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company ", " 4 5 The first two of these lists to be published were the Thesaurus of ASTIA Descriptors 1960 and the Chemical Engineering Thesaurus of the American Institute of Chemical Engineers 1961 a descendant of the Dupont thesaurus ", "More followed culminating in the influential Thesaurus of Engineering and Scientific Terms TEST published jointly by the Engineers Joint Council and the US Department of Defense in 1967 ", "TEST did more than just serve as an example its Appendix 1 presented Thesaurus rules and conventions that have guided thesaurus construction ever since ", "Hundreds of thesauri have been produced since then perhaps thousands ", "The most notable innovations since TEST have been a Extension from monolingual to multilingual capability and b Addition of a conceptually organized display to the basic alphabetical presentation ", "Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST The most clearly visible trend across this history of thesaurus development has been from the context of small scale isolation to a networked world ", " 6 Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications ", "More recently as can be seen from the titles of the latest ISO and NISO standards there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system such as subject heading schemes classification schemes taxonomies and ontologies ", "The official website for ISO 25964 gives more information including a reading list ", " 7 In information retrieval a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities ", "A thesaurus helps with expressing the manifestations of a concept in a prescribed way to aid in improving precision and recall ", "This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language ", "Additionally a thesaurus is used for maintaining a hierarchical listing of terms usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity ", "The Art Architecture Thesaurus for example is used by countless museums around the world to catalogue their collections ", "AGROVOC the thesaurus of the UN s Food and Agriculture Organization is used to index and or search its AGRIS database of worldwide literature on agricultural research ", "Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear ", "For example citrus fruits might be linked to the broader concept of fruits and to the narrower ones of oranges lemons etc ", "When the terms are displayed online the links between them make it very easy to browse the thesaurus selecting useful terms for a search ", "When a single term could have more than one meaning like tables furniture or tables data these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results ", "For any one concept all known synonyms are listed such as mad cow disease bovine spongiform encephalopathy BSE etc ", "The idea is to guide all the indexers and all the searchers to use the same term for the same concept so that search results will be as complete as possible ", "If the thesaurus is multilingual equivalent terms in other languages are shown too ", "Following international standards concepts are generally arranged hierarchically within facets or grouped by themes or topics ", "Unlike a general thesaurus that is used for literary purposes information retrieval thesauri typically focus on one discipline subject or field of study "]},
{"https://en.wikipedia.org/wiki/Bag-of-words_model": ["Pages for logged out editors learn moreIn computer science lexical analysis lexing or tokenization is the process of converting a sequence of characters such as in a computer program or web page into a sequence of lexical tokens strings with an assigned and thus identified meaning ", "A program that performs lexical analysis may be termed a lexer tokenizer 1 or scanner although scanner is also a term for the first stage of a lexer ", "A lexer is generally combined with a parser which together analyze the syntax of programming languages web pages and so forth ", "A lexer forms the first phase of a compiler frontend in processing ", "Analysis generally occurs in one pass ", "In older languages such as ALGOL the initial stage was instead line reconstruction which performed unstropping and removed whitespace and comments and had scannerless parsers with no separate lexer ", "These steps are now done as part of the lexer ", "Lexers and parsers are most often used for compilers but can be used for other computer language tools such as prettyprinters or linters ", "Lexing can be divided into two stages the scanning which segments the input string into syntactic units called lexemes and categorizes these into token classes and the evaluating which converts lexemes into processed values ", "Lexers are generally quite simple with most of the complexity deferred to the parser or semantic analysis phases and can often be generated by a lexer generator notably lex or derivatives ", "However lexers can sometimes include some complexity such as phrase structure processing to make input easier and simplify the parser and may be written partly or fully by hand either to support more features or for performance ", "Lexical analysis is also an important early stage in natural language processing where text or sound waves are segmented into words and other units ", "This requires a variety of decisions which are not fully standardized and the number of tokens systems produce varies for strings like 1 2 chair s can t and or 1 1 2010 2x4 and many others ", "This is in contrast to lexical analysis for programming and similar languages where exact rules are commonly defined and known ", "A lexeme is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token ", " 2 Some authors term this a token using token interchangeably to represent the string being tokenized and the token data structure resulting from putting this string through the tokenization process ", " 3 4 The word lexeme in computer science is defined differently than lexeme in linguistics ", "A lexeme in computer science roughly corresponds to a word in linguistics not to be confused with a word in computer architecture although in some cases it may be more similar to a morpheme ", "In some natural languages for example in English the linguistic lexeme is similar to the lexeme in computer science but this is generally not true for example in Chinese it is highly non trivial to find word boundaries due to the lack of word separators ", "A lexical token or simply token is a string with an assigned and thus identified meaning ", "It is structured as a pair consisting of a token name and an optional token value ", "The token name is a category of lexical unit ", " 2 Common token names areConsider this expression in the C programming language The lexical analysis of this expression yields the following sequence of tokens A token name is what might be termed a part of speech in linguistics ", "The specification of a programming language often includes a set of rules the lexical grammar which defines the lexical syntax ", "The lexical syntax is usually a regular language with the grammar rules consisting of regular expressions they define the set of possible character sequences lexemes of a token ", "A lexer recognizes strings and for each kind of string found the lexical program takes an action most simply producing a token ", "Two important common lexical categories are white space and comments ", "These are also defined in the grammar and processed by the lexer but may be discarded not producing any tokens and considered non significant at most separating two tokens as in if x instead of ifx ", "There are two important exceptions to this ", "First in off side rule languages that delimit blocks with indenting initial whitespace is significant as it determines block structure and is generally handled at the lexer level see phrase structure below ", "Secondly in some uses of lexers comments and whitespace must be preserved for examples a prettyprinter also needs to output the comments and some debugging tools may provide messages to the programmer showing the original source code ", "In the 1960s notably for ALGOL whitespace and comments were eliminated as part of the line reconstruction phase the initial phase of the compiler frontend but this separate phase has been eliminated and these are now handled by the lexer ", "Tokenization is the process of demarcating and possibly classifying sections of a string of input characters ", "The resulting tokens are then passed on to some other form of processing ", "The process can be considered a sub task of parsing input ", "For example in the text string the string isn t implicitly segmented on spaces as a natural language speaker would do ", "The raw input the 43 characters must be explicitly split into the 9 tokens with a given space delimiter i e matching the string or regular expression s 1 ", "When a token class represents more than one possible lexeme the lexer often saves enough information to reproduce the original lexeme so that it can be used in semantic analysis ", "The parser typically retrieves this information from the lexer and stores it in the abstract syntax tree ", "This is necessary in order to avoid information loss in the case where numbers may also be valid identifiers ", "Tokens are identified based on the specific rules of the lexer ", "Some methods used to identify tokens include regular expressions specific sequences of characters termed a flag specific separating characters called delimiters and explicit definition by a dictionary ", "Special characters including punctuation characters are commonly used by lexers to identify tokens because of their natural use in written and programming languages ", "Tokens are often categorized by character content or by context within the data stream ", "Categories are defined by the rules of the lexer ", "Categories often involve grammar elements of the language used in the data stream ", "Programming languages often categorize tokens as identifiers operators grouping symbols or by data type ", "Written languages commonly categorize tokens as nouns verbs adjectives or punctuation ", "Categories are used for post processing of the tokens either by the parser or by other functions in the program ", "A lexical analyzer generally does nothing with combinations of tokens a task left for a parser ", "For example a typical lexical analyzer recognizes parentheses as tokens but does nothing to ensure that each is matched with a ", "When a lexer feeds tokens to the parser the representation used is typically an enumerated list of number representations ", "For example Identifier is represented with 0 Assignment operator with 1 Addition operator with 2 etc ", "Tokens are defined often by regular expressions which are understood by a lexical analyzer generator such as lex ", "The lexical analyzer generated automatically by a tool like lex or hand crafted reads in a stream of characters identifies the lexemes in the stream and categorizes them into tokens ", "This is termed tokenizing ", "If the lexer finds an invalid token it will report an error ", "Following tokenizing is parsing ", "From there the interpreted data may be loaded into data structures for general use interpretation or compiling ", "The first stage the scanner is usually based on a finite state machine FSM ", "It has encoded within it information on the possible sequences of characters that can be contained within any of the tokens it handles individual instances of these character sequences are termed lexemes ", "For example an integer lexeme may contain any sequence of numerical digit characters ", "In many cases the first non whitespace character can be used to deduce the kind of token that follows and subsequent input characters are then processed one at a time until reaching a character that is not in the set of characters acceptable for that token this is termed the maximal munch or longest match rule ", "In some languages the lexeme creation rules are more complex and may involve backtracking over previously read characters ", "For example in C one L character is not enough to distinguish between an identifier that begins with L and a wide character string literal ", "A lexeme however is only a string of characters known to be of a certain kind e g a string literal a sequence of letters ", "In order to construct a token the lexical analyzer needs a second stage the evaluator which goes over the characters of the lexeme to produce a value ", "The lexeme s type combined with its value is what properly constitutes a token which can be given to a parser ", "Some tokens such as parentheses do not really have values and so the evaluator function for these can return nothing only the type is needed ", "Similarly sometimes evaluators can suppress a lexeme entirely concealing it from the parser which is useful for whitespace and comments ", "The evaluators for identifiers are usually simple literally representing the identifier but may include some unstropping ", "The evaluators for integer literals may pass the string on deferring evaluation to the semantic analysis phase or may perform evaluation themselves which can be involved for different bases or floating point numbers ", "For a simple quoted string literal the evaluator needs to remove only the quotes but the evaluator for an escaped string literal incorporates a lexer which unescapes the escape sequences ", "For example in the source code of a computer program the stringmight be converted into the following lexical token stream whitespace is suppressed and special characters have no value Due to licensing restrictions of existing parsers it may be necessary to write a lexer by hand ", "This is practical if the list of tokens is small but in general lexers are generated by automated tools ", "These tools generally accept regular expressions that describe the tokens allowed in the input stream ", "Each regular expression is associated with a production rule in the lexical grammar of the programming language that evaluates the lexemes matching the regular expression ", "These tools may generate source code that can be compiled and executed or construct a state transition table for a finite state machine which is plugged into template code for compiling and executing ", "Regular expressions compactly represent patterns that the characters in lexemes might follow ", "For example for an English based language an IDENTIFIER token might be any English alphabetic character or an underscore followed by any number of instances of ASCII alphanumeric characters and or underscores ", "This could be represented compactly by the string a zA Z a zA Z 0 9 ", "This means any character a z A Z or followed by 0 or more of a z A Z or 0 9 ", "Regular expressions and the finite state machines they generate are not powerful enough to handle recursive patterns such as n opening parentheses followed by a statement followed by n closing parentheses ", "They are unable to keep count and verify that n is the same on both sides unless a finite set of permissible values exists for n It takes a full parser to recognize such patterns in their full generality ", "A parser can push parentheses on a stack and then try to pop them off and see if the stack is empty at the end see example 5 in the Structure and Interpretation of Computer Programs book ", "Typically tokenization occurs at the word level ", "However it is sometimes difficult to define what is meant by a word ", "Often a tokenizer relies on simple heuristics for example In languages that use inter word spaces such as most that use the Latin alphabet and most programming languages this approach is fairly straightforward ", "However even here there are many edge cases such as contractions hyphenated words emoticons and larger constructs such as URIs which for some purposes may count as single tokens ", "A classic example is New York based which a naive tokenizer may break at the space even though the better break is arguably at the hyphen ", "Tokenization is particularly difficult for languages written in scriptio continua which exhibit no word boundaries such as Ancient Greek Chinese 6 or Thai ", "Agglutinative languages such as Korean also make tokenization tasks complicated ", "Some ways to address the more difficult problems include developing more complex heuristics querying a table of common special cases or fitting the tokens to a language model that identifies collocations in a later processing step ", "Lexers are often generated by a lexer generator analogous to parser generators and such tools often come together ", "The most established is lex paired with the yacc parser generator or rather some of their many reimplementations like flex often paired with GNU Bison ", "These generators are a form of domain specific language taking in a lexical specification generally regular expressions with some markup and emitting a lexer ", "These tools yield very fast development which is very important in early development both to get a working lexer and because a language specification may change often ", "Further they often provide advanced features such as pre and post conditions which are hard to program by hand ", "However an automatically generated lexer may lack flexibility and thus may require some manual modification or an all manually written lexer ", "Lexer performance is a concern and optimizing is worthwhile more so in stable languages where the lexer is run very often such as C or HTML ", "lex flex generated lexers are reasonably fast but improvements of two to three times are possible using more tuned generators ", "Hand written lexers are sometimes used but modern lexer generators produce faster lexers than most hand coded ones ", "The lex flex family of generators uses a table driven approach which is much less efficient than the directly coded approach ", " dubious discuss With the latter approach the generator produces an engine that directly jumps to follow up states via goto statements ", "Tools like re2c 7 have proven to produce engines that are between two and three times faster than flex produced engines ", " citation needed It is in general difficult to hand write analyzers that perform better than engines generated by these latter tools ", "Lexical analysis mainly segments the input stream of characters into tokens simply grouping the characters into pieces and categorizing them ", "However the lexing may be significantly more complex most simply lexers may omit tokens or insert added tokens ", "Omitting tokens notably whitespace and comments is very common when these are not needed by the compiler ", "Less commonly added tokens may be inserted ", "This is done mainly to group tokens into statements or statements into blocks to simplify the parser ", "Line continuation is a feature of some languages where a newline is normally a statement terminator ", "Most often ending a line with a backslash immediately followed by a newline results in the line being continued the following line is joined to the prior line ", "This is generally done in the lexer the backslash and newline are discarded rather than the newline being tokenized ", "Examples include bash 8 other shell scripts and Python ", " 9 Many languages use the semicolon as a statement terminator ", "Most often this is mandatory but in some languages the semicolon is optional in many contexts ", "This is mainly done at the lexer level where the lexer outputs a semicolon into the token stream despite one not being present in the input character stream and is termed semicolon insertion or automatic semicolon insertion ", "In these cases semicolons are part of the formal phrase grammar of the language but may not be found in input text as they can be inserted by the lexer ", "Optional semicolons or other terminators or separators are also sometimes handled at the parser level notably in the case of trailing commas or semicolons ", "Semicolon insertion is a feature of BCPL and its distant descendant Go 10 though it is absent in B or C 11 Semicolon insertion is present in JavaScript though the rules are somewhat complex and much criticized to avoid bugs some recommend always using semicolons while others use initial semicolons termed defensive semicolons at the start of potentially ambiguous statements ", "Semicolon insertion in languages with semicolon terminated statements and line continuation in languages with newline terminated statements can be seen as complementary semicolon insertion adds a token even though newlines generally do not generate tokens while line continuation prevents a token from being generated even though newlines generally do generate tokens ", "The off side rule blocks determined by indenting can be implemented in the lexer as in Python where increasing the indenting results in the lexer emitting an INDENT token and decreasing the indenting results in the lexer emitting a DEDENT token ", " 9 These tokens correspond to the opening brace and closing brace in languages that use braces for blocks and means that the phrase grammar does not depend on whether braces or indenting are used ", "This requires that the lexer hold state namely the current indent level and thus can detect changes in indenting when this changes and thus the lexical grammar is not context free INDENT DEDENT depend on the contextual information of prior indent level ", "Generally lexical grammars are context free or almost so and thus require no looking back or ahead or backtracking which allows a simple clean and efficient implementation ", "This also allows simple one way communication from lexer to parser without needing any information flowing back to the lexer ", "There are exceptions however ", "Simple examples include semicolon insertion in Go which requires looking back one token concatenation of consecutive string literals in Python 9 which requires holding one token in a buffer before emitting it to see if the next token is another string literal and the off side rule in Python which requires maintaining a count of indent level indeed a stack of each indent level ", "These examples all only require lexical context and while they complicate a lexer somewhat they are invisible to the parser and later phases ", "A more complex example is the lexer hack in C where the token class of a sequence of characters cannot be determined until the semantic analysis phase since typedef names and variable names are lexically identical but constitute different token classes ", "Thus in the hack the lexer calls the semantic analyzer say symbol table and checks if the sequence requires a typedef name ", "In this case information must flow back not from the parser only but from the semantic analyzer back to the lexer which complicates design "]},
{"https://en.wikipedia.org/wiki/Spell_checker": ["Pages for logged out editors learn moreThe bag of words model is a simplifying representation used in natural language processing and information retrieval IR ", "In this model a text such as a sentence or a document is represented as the bag multiset of its words disregarding grammar and even word order but keeping multiplicity ", "The bag of words model has also been used for computer vision ", " 1 The bag of words model is commonly used in methods of document classification where the frequency of occurrence of each word is used as a feature for training a classifier ", " 2 An early reference to bag of words in a linguistic context can be found in Zellig Harris s 1954 article on Distributional Structure ", " 3 The Bag of words model is one example of a Vector space model ", "The following models a text document using bag of words ", "Here are two simple text documents Based on these two text documents a list is constructed as follows for each document Representing each bag of words as a JSON object and attributing to the respective JavaScript variable Each key is the word and each value is the number of occurrences of that word in the given text document ", "The order of elements is free so for example too 1 Mary 1 movies 2 John 1 watch 1 likes 2 to 1 is also equivalent to BoW1 ", "It is also what we expect from a strict JSON object representation ", "Note if another document is like a union of these two its JavaScript representation will be So as we see in the bag algebra the union of two documents in the bags of words representation is formally the disjoint union summing the multiplicities of each element ", "B o W 3 B o W 1 B o W 2 displaystyle BoW3 BoW1 biguplus BoW2 ", "In practice the Bag of words model is mainly used as a tool of feature generation ", "After transforming the text into a bag of words we can calculate various measures to characterize the text ", "The most common type of characteristics or features calculated from the Bag of words model is term frequency namely the number of times a term appears in the text ", "For the example above we can construct the following two lists to record the term frequencies of all the distinct words BoW1 and BoW2 ordered as in BoW3 Each entry of the lists refers to the count of the corresponding entry in the list this is also the histogram representation ", "For example in the first list which represents document 1 the first two entries are 1 2 This list or vector representation does not preserve the order of the words in the original sentences ", "This is just the main feature of the Bag of words model ", "This kind of representation has several successful applications such as email filtering ", " 1 However term frequencies are not necessarily the best representation for the text ", "Common words like the a to are almost always the terms with highest frequency in the text ", "Thus having a high raw count does not necessarily mean that the corresponding word is more important ", "To address this problem one of the most popular ways to normalize the term frequencies is to weight a term by the inverse of document frequency or tf idf ", "Additionally for the specific purpose of classification supervised alternatives have been developed to account for the class label of a document ", " 4 Lastly binary presence absence or 1 0 weighting is used in place of frequencies for some problems e g this option is implemented in the WEKA machine learning software system ", "The Bag of words model is an orderless document representation only the counts of words matter ", "For instance in the above example John likes to watch movies ", "Mary likes movies too the bag of words representation will not reveal that the verb likes always follows a person s name in this text ", "As an alternative the n gram model can store this spatial information ", "Applying to the same example above a bigram model will parse the text into the following units and store the term frequency of each unit as before ", "Conceptually we can view bag of word model as a special case of the n gram model with n 1 ", "For n 1 the model is named w shingling where w is equivalent to n denoting the number of grouped words ", "See language model for a more detailed discussion ", "A common alternative to using dictionaries is the hashing trick where words are mapped directly to indices with a hashing function ", " 5 Thus no memory is required to store a dictionary ", "Hash collisions are typically dealt via freed up memory to increase the number of hash buckets ", "In practice hashing simplifies the implementation of bag of words models and improves scalability ", "In Bayesian spam filtering an e mail message is modeled as an unordered collection of words selected from one of two probability distributions one representing spam and one representing legitimate e mail ham ", "Imagine there are two literal bags full of words ", "One bag is filled with words found in spam messages and the other with words found in legitimate e mail ", "While any given word is likely to be somewhere in both bags the spam bag will contain spam related words such as stock Viagra and buy significantly more frequently while the ham bag will contain more words related to the user s friends or workplace ", "To classify an e mail message the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags and uses Bayesian probability to determine which bag it is more likely to be in "]},
{"https://en.wikipedia.org/wiki/Deep_linguistic_processing": ["Pages for logged out editors learn moreIn software a spell checker or spelling checker or spell check is a software feature that checks for misspellings in a text ", "Spell checking features are often embedded in software or services such as a word processor email client electronic dictionary or search engine ", "Eye have a spelling chequer It came with my Pea Sea ", "It plane lee marks four my revueMiss Steaks I can knot sea ", "Eye strike the quays and type a whirred And weight four it two say Weather eye am write oar wrong It tells me straight a weigh ", "Eye ran this poem threw it Your shore real glad two no ", "Its vary polished in its weigh ", "My chequer tolled me sew ", "A chequer is a bless thing It freeze yew lodes of thyme ", "It helps me right all stiles of righting And aides me when eye rime ", "Each frays come posed up on my screenEye trussed too bee a joule ", "The chequer pours o er every wordTwo cheque sum spelling rule ", "A basic spell checker carries out the following processes It is unclear whether morphological analysis allowing for many forms of a word depending on its grammatical role provides a significant benefit for English though its benefits for highly synthetic languages such as German Hungarian or Turkish are clear ", "As an adjunct to these components the program s user interface allows users to approve or reject replacements and modify the program s operation ", "Spell checkers can use approximate string matching algorithms such as Levenshtein distance to find correct spellings of misspelled words ", " 1 An alternative type of spell checker uses solely statistical information such as n grams to recognize errors instead of correctly spelled words ", "This approach usually requires a lot of effort to obtain sufficient statistical information ", "Key advantages include needing less runtime storage and the ability to correct errors in words that are not included in a dictionary ", " 2 In some cases spell checkers use a fixed list of misspellings and suggestions for those misspellings this less flexible approach is often used in paper based correction methods such as the see also entries of encyclopedias ", "Clustering algorithms have also been used for spell checking 3 combined with phonetic information ", " 4 In 1961 Les Earnest who headed the research on this budding technology saw it necessary to include the first spell checker that accessed a list of 10 000 acceptable words ", " 5 Ralph Gorin a graduate student under Earnest at the time created the first true spelling checker program written as an applications program rather than research for general English text SPELL for the DEC PDP 10 at Stanford University s Artificial Intelligence Laboratory in February 1971 ", " 6 Gorin wrote SPELL in assembly language for faster action he made the first spelling corrector by searching the word list for plausible correct spellings that differ by a single letter or adjacent letter transpositions and presenting them to the user ", "Gorin made SPELL publicly accessible as was done with most SAIL Stanford Artificial Intelligence Laboratory programs and it soon spread around the world via the new ARPAnet about ten years before personal computers came into general use ", " 7 SPELL its algorithms and data structures inspired the Unix ispell program ", "The first spell checkers were widely available on mainframe computers in the late 1970s ", "A group of six linguists from Georgetown University developed the first spell check system for the IBM corporation ", " 8 Henry Ku era invented one for the VAX machines of Digital Equipment Corp in 1981 ", " 9 The first spell checkers for personal computers appeared in 1980 such as WordCheck for Commodore systems which was released in late 1980 in time for advertisements to go to print in January 1981 ", " 10 Developers such as Maria Mariani 8 and Random House 11 rushed OEM packages or end user products into the rapidly expanding software market ", "On the pre Windows PCs these spell checkers were standalone programs many of which could be run in terminate and stay resident mode from within word processing packages on PCs with sufficient memory ", "However the market for standalone packages was short lived as by the mid 1980s developers of popular word processing packages like WordStar and WordPerfect had incorporated spell checkers in their packages mostly licensed from the above companies who quickly expanded support from just English to many European and eventually even Asian languages ", "However this required increasing sophistication in the morphology routines of the software particularly with regard to heavily agglutinative languages like Hungarian and Finnish ", "Although the size of the word processing market in a country like Iceland might not have justified the investment of implementing a spell checker companies like WordPerfect nonetheless strove to localize their software for as many national markets as possible as part of their global marketing strategy ", "When Apple developed a system wide spelling checker for Mac OS X so that the operating system took over spelling fixes 12 it was a first one didn t have to maintain a separate spelling checker for each program ", " 13 Mac OS X s spellcheck coverage includes virtually all bundled and third party applications ", "Visual Tools VT Speller introduced in 1994 was designed for developers of applications that support Windows ", " 14 15 It came with a dictionary but had the ability to build and incorporate use of secondary dictionaries ", " 16 Firefox 2 0 a web browser has spell check support for user written content 17 such as when editing Wikitext writing on many webmail sites blogs and social networking websites ", "The web browsers Google Chrome Konqueror and Opera the email client Kmail and the instant messaging client Pidgin also offer spell checking support transparently using previously GNU Aspell and currently Hunspell as their engine ", "Some spell checkers have separate support for medical dictionaries to help prevent medical errors ", " 18 19 20 The first spell checkers were verifiers instead of correctors ", "They offered no suggestions for incorrectly spelled words ", "This was helpful for typos but it was not so helpful for logical or phonetic errors ", "The challenge the developers faced was the difficulty in offering useful suggestions for misspelled words ", "This requires reducing words to a skeletal form and applying pattern matching algorithms ", "It might seem logical that where spell checking dictionaries are concerned the bigger the better so that correct words are not marked as incorrect ", "In practice however an optimal size for English appears to be around 90 000 entries ", "If there are more than this incorrectly spelled words may be skipped because they are mistaken for others ", "For example a linguist might determine on the basis of corpus linguistics that the word baht is more frequently a misspelling of bath or bat than a reference to the Thai currency ", "Hence it would typically be more useful if a few people who write about Thai currency were slightly inconvenienced than if the spelling errors of the many more people who discuss baths were overlooked ", "The first MS DOS spell checkers were mostly used in proofing mode from within word processing packages ", "After preparing a document a user scanned the text looking for misspellings ", "Later however batch processing was offered in such packages as Oracle s short lived CoAuthor and allowed a user to view the results after a document was processed and correct only the words that were known to be wrong ", "When memory and processing power became abundant spell checking was performed in the background in an interactive way such as has been the case with the Sector Software produced Spellbound program released in 1987 and Microsoft Word since Word 95 ", "Spell checkers became increasingly sophisticated now capable of recognizing grammatical errors ", "However even at their best they rarely catch all the errors in a text such as homophone errors and will flag neologisms and foreign words as misspellings ", "Nonetheless spell checkers can be considered as a type of foreign language writing aid that non native language learners can rely on to detect and correct their misspellings in the target language ", " 21 English is unusual in that most words used in formal writing have a single spelling that can be found in a typical dictionary with the exception of some jargon and modified words ", "In many languages words are often concatenated into new combinations of words ", "In German compound nouns are frequently coined from other existing nouns ", "Some scripts do not clearly separate one word from another requiring word splitting algorithms ", "Each of these presents unique challenges to non English language spell checkers ", "There has been research on developing algorithms that are capable of recognizing a misspelled word even if the word itself is in the vocabulary based on the context of the surrounding words ", "Not only does this allow words such as those in the poem above to be caught but it mitigates the detrimental effect of enlarging dictionaries allowing more words to be recognized ", "For example baht in the same paragraph as Thai or Thailand would not be recognized as a misspelling of bath ", "The most common example of errors caught by such a system are homophone errors such as the bold words in the following sentence The most successful algorithm to date is Andrew Golding and Dan Roth s Winnow based spelling correction algorithm 22 published in 1999 which is able to recognize about 96 of context sensitive spelling errors in addition to ordinary non word spelling errors ", "Context sensitive spell checkers appeared in the now defunct applications Microsoft Office 2007 23 and Google Wave ", " 24 Grammar checkers attempt to fix problems with grammar beyond spelling errors including incorrect choice of words "]},
{"https://en.wikipedia.org/wiki/Text_corpus": ["Pages for logged out editors learn moreDeep linguistic processing is a natural language processing framework which draws on theoretical and descriptive linguistics ", "It models language predominantly by way of theoretical syntactic semantic theory e g ", "CCG HPSG LFG TAG the Prague School ", "Deep linguistic processing approaches differ from shallower methods in that they yield more expressive and structural representations which directly capture long distance dependencies and underlying predicate argument structures ", " 1 The knowledge intensive approach of deep linguistic processing requires considerable computational power and has in the past sometimes been judged as being intractable ", "However research in the early 2000s had made considerable advancement in efficiency of deep processing ", " 2 3 Today efficiency is no longer a major problem for applications using deep linguistic processing ", "Traditionally deep linguistic processing has been concerned with computational grammar development for use in both parsing and generation ", "These grammars were manually developed maintained and were computationally expensive to run ", "In recent years machine learning approaches also known as shallow linguistic processing have fundamentally altered the field of natural language processing ", "The rapid creation of robust and wide coverage machine learning NLP tools requires substantially lesser amount of manual labor ", "Thus deep linguistic processing methods have received less attention ", "However it is the belief of some computational linguists who ", "that in order for computers to understand natural language or inference detailed syntactic and semantic representation is necessary ", "Moreover while humans can easily understand a sentence and its meaning shallow linguistic processing might lack human language understanding ", "For example 4 In sentence a a shallow information extraction system might infer wrongly that Microsoft s headquarters was located in Georgia ", "While as humans we understand from the sentence that Microsoft office was never in Georgia ", "In sentence b a shallow system could wrongly infer that Israel was established in May 1971 ", "Humans know that it is the National Institute for Psychobiology that was established in 1971 ", "In summary of the comparison between deep and shallow language processing deep linguistic processing provides a knowledge rich analysis of language through manually developed grammars and language resources ", "Whereas shallow linguistic processing provides a knowledge lean analysis of language through statistical machine learning manipulation of texts and or annotated linguistic resource ", " Deep computational linguists are divided in different sub communities based on the grammatical formalism they adopted for deep linguistic processing ", "The major sub communities includes the The shortlist above is not exhaustively representative of all the communities working on deep linguistic processing "]},
{"https://en.wikipedia.org/wiki/Grammar_checker": ["Pages for logged out editors learn moreIn linguistics a corpus plural corpora or text corpus is a language resource consisting of a large and structured set of texts nowadays usually electronically stored and processed ", "In corpus linguistics they are used to do statistical analysis and hypothesis testing checking occurrences or validating linguistic rules within a specific language territory ", "In search technology a corpus is the collection of documents which is being searched ", "A corpus may contain texts in a single language monolingual corpus or text data in multiple languages multilingual corpus ", "In order to make the corpora more useful for doing linguistic research they are often subjected to a process known as annotation ", "An example of annotating a corpus is part of speech tagging or POS tagging in which information about each word s part of speech verb noun adjective etc ", "is added to the corpus in the form of tags ", "Another example is indicating the lemma base form of each word ", "When the language of the corpus is not a working language of the researchers who use it interlinear glossing is used to make the annotation bilingual ", "Some corpora have further structured levels of analysis applied ", "In particular smaller corpora may be fully parsed ", "Such corpora are usually called Treebanks or Parsed Corpora ", "The difficulty of ensuring that the entire corpus is completely and consistently annotated means that these corpora are usually smaller containing around one to three million words ", "Other levels of linguistic structured analysis are possible including annotations for morphology semantics and pragmatics ", "Corpora are the main knowledge base in corpus linguistics ", "Other notable areas of application include "]},
{"https://en.wikipedia.org/wiki/Natural_language_generation": ["Pages for logged out editors learn moreA grammar checker in computing terms is a program or part of a program that attempts to verify written text for grammatical correctness ", "Grammar checkers are most often implemented as a feature of a larger program such as a word processor but are also available as a stand alone application that can be activated from within programs that work with editable text ", "The implementation of a grammar checker makes use of natural language processing ", " 1 2 The earliest grammar checkers were programs that checked for punctuation and style inconsistencies rather than a complete range of possible grammatical errors ", "The first system was called Writer s Workbench and was a set of writing tools included with Unix systems as far back as the 1970s ", " 3 4 The whole Writer s Workbench package included several separate tools to check for various writing problems ", "The diction tool checked for wordy trite clich d or misused phrases in a text ", "The tool would output a list of questionable phrases and provide suggestions for improving the writing ", "The style tool analyzed the writing style of a given text ", "It performed a number of readability tests on the text and output the results and gave some statistical information about the sentences of the text ", "Aspen Software of Albuquerque New Mexico released the earliest version of a diction and style checker for personal computers Grammatik in 1981 ", "Grammatik was first available for a Radio Shack TRS 80 and soon had versions for CP M and the IBM PC ", "Reference Software of San Francisco California acquired Grammatik in 1985 ", "Development of Grammatik continued and it became an actual grammar checker that could detect writing errors beyond simple style checking ", "Other early diction and style checking programs included Punctuation Style Correct Grammar RightWriter and PowerEdit ", " 5 While all the earliest programs started out as simple diction and style checkers all eventually added various levels of language processing and developed some level of true grammar checking capability ", "Until 1992 grammar checkers were sold as add on programs ", "There were a large number of different word processing programs available at that time with WordPerfect and Microsoft Word the top two in market share ", "In 1992 Microsoft decided to add grammar checking as a feature of Word and licensed CorrecText a grammar checker from Houghton Mifflin that had not yet been marketed as a standalone product ", "WordPerfect answered Microsoft s move by acquiring Reference Software and the direct descendant of Grammatik is still included with WordPerfect ", "As of 2019 grammar checkers are built into systems like Google Docs and Sapling ai 6 browser extensions like Grammarly and Qordoba desktop applications like Ginger free and open source software like LanguageTool 7 and text editor plugins like those available from WebSpellChecker Software ", "The earliest writing style programs checked for wordy trite clich d or misused phrases in a text ", "This process was based on simple pattern matching ", "The heart of the program was a list of many hundreds or thousands of phrases that are considered poor writing by many experts ", "The list of questionable phrases included alternative wording for each phrase ", "The checking program would simply break text into sentences check for any matches in the phrase dictionary flag suspect phrases and show an alternative ", "These programs could also perform some mechanical checks ", "For example they would typically flag doubled words doubled punctuation some capitalization errors and other simple mechanical mistakes ", "True grammar checking is more complex ", "While a programming language has a very specific syntax and grammar this is not so for natural languages ", "One can write a somewhat complete formal grammar for a natural language but there are usually so many exceptions in real usage that a formal grammar is of minimal help in writing a grammar checker ", "One of the most important parts of a natural language grammar checker is a dictionary of all the words in the language along with the part of speech of each word ", "The fact that a natural word may be used as any one of several parts of speech such as free being used as an adjective adverb noun or verb greatly increases the complexity of any grammar checker ", "A grammar checker will find each sentence in a text look up each word in the dictionary and then attempt to parse the sentence into a form that matches a grammar ", "Using various rules the program can then detect various errors such as agreement in tense number word order and so on ", "It is also possible to detect some stylistic problems with the text ", "For example some popular style guides such as The Elements of Style deprecate excessive use of the passive voice ", "Grammar checkers may attempt to identify passive sentences and suggest an active voice alternative ", "The software elements required for grammar checking are closely related to some of the development issues that need to be addressed for speech recognition software ", "In voice recognition parsing can be used to help predict which word is most likely intended based on part of speech and position in the sentence ", "In grammar checking the parsing is used to detect words that fail to follow accepted grammar usage ", "Recently when ", "research has focused on developing algorithms which can recognize grammar errors based on the context of the surrounding words ", " clarification needed Grammar checkers are considered as a type of foreign language writing aid which non native speakers can use to proofread their writings as such programs endeavor to identify syntactical errors ", " 8 However as with other computerized writing aids such as spell checkers popular grammar checkers are often criticized when they fail to spot errors and incorrectly flag correct text as erroneous ", "The linguist Geoffrey K Pullum argued in 2007 that they were generally so inaccurate as to do more harm than good for the most part accepting the advice of a computer grammar checker on your prose will make it much worse sometimes hilariously incoherent ", " 9 "]},
{"https://en.wikipedia.org/wiki/BLEU": ["Pages for logged out editors learn moreNatural language generation NLG is a software process that produces natural language output ", "A widely cited survey of NLG methods describes NLG as the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems than can produce understandable texts in English or other human languages from some underlying non linguistic representation of information ", " 1 While it is widely agreed that the output of any NLG process is text there is some disagreement about whether the inputs of an NLG system need to be non linguistic ", " 2 Common applications of NLG methods include the production of various reports for example weather 3 and patient reports 4 image captions 5 and chatbots ", "Automated NLG can be compared to the process humans use when they turn ideas into writing or speech ", "Psycholinguists prefer the term language production for this process which can also be described in mathematical terms or modeled in a computer for psychological research ", "NLG systems can also be compared to translators of artificial computer languages such as decompilers or transpilers which also produce human readable code generated from an intermediate representation ", "Human languages tend to be considerably more complex and allow for much more ambiguity and variety of expression than programming languages which makes NLG more challenging ", "NLG may be viewed as complementary to natural language understanding NLU whereas in natural language understanding the system needs to disambiguate the input sentence to produce the machine representation language in NLG the system needs to make decisions about how to put a representation into words ", "The practical considerations in building NLU vs NLG systems are not symmetrical ", "NLU needs to deal with ambiguous or erroneous user input whereas the ideas the system wants to express through NLG are generally known precisely ", "NLG needs to choose a specific self consistent textual representation from many potential representations whereas NLU generally tries to produce a single normalized representation of the idea expressed ", " 6 NLG has existed since ELIZA was developed in the mid 1960s but the methods were first used commercially in the 1990s ", " 7 NLG techniques range from simple template based systems like a mail merge that generates form letters to systems that have a complex understanding of human grammar ", "NLG can also be accomplished by training a statistical model using machine learning typically on a large corpus of human written texts ", " 8 The Pollen Forecast for Scotland system 9 is a simple example of a simple NLG system that could essentially be a template ", "This system takes as input six numbers which give predicted pollen levels in different parts of Scotland ", "From these numbers the system generates a short textual summary of pollen levels as its output ", "For example using the historical data for July 1 2005 the software produces Grass pollen levels for Friday have increased from the moderate to high levels of yesterday with values of around 6 to 7 across most parts of the country ", "However in Northern areas pollen levels will be moderate with values of 4 ", "In contrast the actual forecast written by a human meteorologist from this data was Pollen counts are expected to remain high at level 6 over most of Scotland and even level 7 in the south east ", "The only relief is in the Northern Isles and far northeast of mainland Scotland with medium levels of pollen count ", "Comparing these two illustrates some of the choices that NLG systems must make these are further discussed below ", "The process to generate text can be as simple as keeping a list of canned text that is copied and pasted possibly linked with some glue text ", "The results may be satisfactory in simple domains such as horoscope machines or generators of personalised business letters ", "However a sophisticated NLG system needs to include stages of planning and merging of information to enable the generation of text that looks natural and does not become repetitive ", "The typical stages of natural language generation as proposed by Dale and Reiter 6 are Content determination Deciding what information to mention in the text ", "For instance in the pollen example above deciding whether to explicitly mention that pollenlevel is 7 in the south east ", "Document structuring Overall organisation of the information to convey ", "For example deciding todescribe the areas with high pollen levels first instead of the areas with low pollen levels ", "Aggregation Merging of similar sentences to improve readability and naturalness ", "For instance merging the two following sentences into the following single sentence Lexical choice Putting words to the concepts ", "For example deciding whether medium or moderateshould be used when describing a pollen level of 4 ", "Referring expression generation Creating referring expressions that identify objects and regions ", "For example deciding to usein the Northern Isles and far northeast of mainland Scotland to refer to a certain region in Scotland ", "This task also includes making decisions about pronouns and other types ofanaphora ", "Realization Creating the actual text which should be correctaccording to the rules ofsyntax morphology and orthography ", "For example using will be for the futuretense of to be ", "An alternative approach to NLG is to use end to end machine learning to build a system without having separate stages as above ", " 10 In other words we build an NLG system by training a machine learning algorithm often an LSTM on a large data set of input data and corresponding human written output texts ", "The end to end approach has perhaps been most successful in image captioning 11 that is automatically generating a textual caption for an image ", "From a commercial perspective the most successful NLG applicationshave been data to text systems which generate textual summaries of databases and data sets thesesystems usually perform data analysis as well as text generation ", "Research has shown that textual summaries can be more effective than graphs and other visuals for decision support 12 13 14 and that computer generated texts can be superior from the reader s perspective to human written texts ", " 15 The first commercial data to text systems produced weather forecasts from weather data ", "The earliest such system to be deployed was FoG 3 which was used by Environment Canada to generate weather forecasts in French and English in the early 1990s ", "The success of FoG triggered other work both research and commercial ", "Recent applications include the UK Met Office s text enhanced forecast ", " 16 Data to text systems have since been applied in a range of settings ", "Following the minor earthquake near Beverly Hills California on March 17 2014 The Los Angeles Times reported details about the time location and strength of the quake within 3 minutes of the event ", "This report was automatically generated by a robo journalist which converted the incoming data into text via a preset template ", " 17 18 Currently there is considerable commercial interest in using NLG to summarise financial and business data ", "Indeed Gartner has said that NLG will become a standard feature of 90 of modern BI and analytics platforms ", " 19 NLG is also being used commercially in automated journalism chatbots generating product descriptions for e commerce sites summarising medical records 20 4 and enhancing accessibility for example by describing graphs and data sets to blind people 21 ", "An example of an interactive use of NLG is the WYSIWYM framework ", "It stands for What you see is what you meant and allows users to see and manipulate the continuously rendered view NLG output of an underlying formal language document NLG input thereby editing the formal language without learning it ", "Looking ahead the current progress in data to text generation paves the way for tailoring texts to specific audiences ", "For example data from babies in neonatal care can be converted into text differently in a clinical setting with different levels of technical detail and explanatory language depending on intended recipient of the text doctor nurse patient ", "The same idea can be applied in a sports setting with different reports generated for fans of specific teams ", " 22 Over the past few years there has been an increased interest in automatically generating captions for images as part of a broader endeavor to investigate the interface between vision and language ", "A case of data to text generation the algorithm of image captioning or automatic image description involves taking an image analyzing its visual content and generating a textual description typically a sentence that verbalizes the most prominent aspects of the image ", "An image captioning system involves two sub tasks ", "In Image Analysis features and attributes of an image are detected and labelled before mapping these outputs to linguistic structures ", "Recent research utilizes deep learning approaches through features from a pre trained convolutional neural network such as AlexNet VGG or Caffe where caption generators use an activation layer from the pre trained network as their input features ", "Text Generation the second task is performed using a wide range of techniques ", "For example in the Midge system input images are represented as triples consisting of object stuff detections action pose detections and spatial relations ", "These are subsequently mapped to noun verb preposition triples and realized using a tree substitution grammar ", " 22 Despite advancements challenges and opportunities remain in image capturing research ", "Notwithstanding the recent introduction of Flickr30K MS COCO and other large datasets have enabled the training of more complex models such as neural networks it has been argued that research in image captioning could benefit from larger and diversified datasets ", "Designing automatic measures that can mimic human judgments in evaluating the suitability of image descriptions is another need in the area ", "Other open challenges include visual question answering VQA 23 as well as the construction and evaluation multilingual repositories for image description ", " 22 Another area where NLG has been widely applied is automated dialogue systems frequently in the form of chatbots ", "A chatbot or chatterbot is a software application used to conduct an on line chat conversation via text or text to speech in lieu of providing direct contact with a live human agent ", "While natural language processing NLP techniques are applied in deciphering human input NLG informs the output part of the chatbot algorithms in facilitating real time dialogues ", "Early chatbot systems including CleverBot created by Rollo Carpenter in 1988 and published in 1997 citation needed reply to questions by identifying how a human has responded to the same question in a conversation database using information retrieval IR techniques ", " citation needed Modern chatbot systems predominantly rely on machine learning ML models such as sequence to sequence learning and reinforcement learning to generate natural language output ", "Hybrid models have also been explored ", "For example the Alibaba shopping assistant first uses an IR approach to retrieve the best candidates from the knowledge base then uses the ML driven seq2seq model re rank the candidate responses and generate the answer ", " 24 Creative language generation by NLG has been hypothesized since the field s origins ", "A recent pioneer in the area is Phillip Parker who has developed an arsenal of algorithms capable of automatically generating textbooks crossword puzzles poems and books on topics ranging from bookbinding to cataracts ", " 25 The advent of large pretrained transformer based language models such as GPT 3 has also enabled breakthroughs with such models demonstrating recognizable ability for creating writing tasks ", " 26 A related area of NLG application is computational humor production ", "JAPE Joke Analysis and Production Engine is one of the earliest large automated humor production systems that uses a hand coded template based approach to create punning riddles for children ", "HAHAcronym creates humorous reinterpretations of any given acronym as well as proposing new fitting acronyms given some keywords ", " 27 Despite progresses many challenges remain in producing automated creative and humorous content that rival human output ", "In an experiment for generating satirical headlines outputs of their best BERT based model were perceived as funny 9 4 of the time while real Onion headlines were 38 4 and a GPT 2 model fine tuned on satirical headlines achieved 6 9 ", " 28 It has been pointed out that two main issues with humor generation systems are the lack of annotated data sets and the lack of formal evaluation methods 27 which could be applicable to other creative content generation ", "Some have argued relative to other applications there has been a lack of attention to creative aspects of language production within NLG ", "NLG researchers stand to benefit from insights into what constitutes creative language production as well as structural features of narrative that have the potential to improve NLG output even in data to text systems ", " 22 As in other scientific fields NLG researchers need to test how well their systems modules and algorithms work ", "This is called evaluation ", "There are three basic techniques for evaluating NLG systems An ultimate goal is how useful NLG systems are at helping people which is the first of the above techniques ", "However task based evaluations are time consuming and expensive and can be difficult to carry out especially if they require subjects with specialised expertise such as doctors ", "Hence as in other areas of NLP task based evaluations are the exception not the norm ", "Recently researchers are assessing how well human ratings and metrics correlate with predict task based evaluations ", "Work is being conducted in the context of Generation Challenges 29 shared task events ", "Initial results suggest that human ratings are much better than metrics in this regard ", "In other words human ratings usually do predict task effectiveness at least to some degree although there are exceptions while ratings produced by metrics often do not predict task effectiveness well ", "These results are preliminary ", "In any case human ratings are the most popular evaluation technique in NLG this is contrast to machine translation where metrics are widely used ", "An AI can be graded on faithfulness to its training data or alternatively on factuality ", "A response that reflects the training data but not reality is faithful but not factual ", "A confident but unfaithful response is a hallucination ", "In Natural Language Processing a hallucination is often defined as generated content that is nonsensical or unfaithful to the provided source content ", " 30 "]}
]